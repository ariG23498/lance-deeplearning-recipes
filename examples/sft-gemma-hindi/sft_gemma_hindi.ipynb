{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4W5RwnR_RSs"
      },
      "source": [
        "For more information on the dataset, see the dolly-alpaca-hindi datasets page [here](https://www.kaggle.com/datasets/heyytanay/dolly-alpaca-hindi)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q0bBGltJ2-k"
      },
      "outputs": [],
      "source": [
        "import lance\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b41oyZqpKHRS"
      },
      "outputs": [],
      "source": [
        "# In this example we are fine-tuning the Gemma-2b model but you can change it to any model of your choice\n",
        "model_id = \"google/gemma-2b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JBOKy8O-1ww"
      },
      "outputs": [],
      "source": [
        "# Define a Prompt prefix, middle and suffix that we will pre-tokenize (to save redundant computation)\n",
        "# and them arrange to include the actual instructions, inputs and outputs.\n",
        "PROMPT_PRE = \"\"\"नीचे एक निर्देश है जो किसी कार्य का वर्णन करता है, जिसे एक इनपुट के साथ जोड़ा गया है जो आगे का संदर्भ प्रदान करता है। एक प्रतिक्रिया लिखें जो अनुरोध को उचित रूप से पूरा करती है।\\n### निर्देश:\\n\"\"\"\n",
        "PROMPT_MID = \"\"\"\\n### इनपुट:\\n\"\"\"\n",
        "PROMPT_SUF = \"\"\"\\n### प्रतिक्रिया:\\n\"\"\"\n",
        "pre_tok = tokenizer(PROMPT_PRE)['input_ids']\n",
        "mid_tok = tokenizer(PROMPT_MID)['input_ids']\n",
        "suf_tok = tokenizer(PROMPT_SUF)['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZiGOwdMKJ8j"
      },
      "outputs": [],
      "source": [
        "class LanceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset that does the following:\n",
        "     - Load instructions, inputs and outputs from a Lance dataset\n",
        "     - Truncates them to a cutoff length (this is to stop an exceptionally long example from crashing our training)\n",
        "     - Arrange them to be in the right format by adding them in a prompt (pre-tokenized, in the above cell)\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, pad_tok_id=None, cutoff=None):\n",
        "        self.ds = lance.dataset(dataset)\n",
        "        # Default cutoff length is 128 tokens and padding token id is 0\n",
        "        self.cutoff = cutoff if cutoff else 128\n",
        "        self.pad_tok_id = pad_tok_id if pad_tok_id else 0\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the data at the current index as a list\n",
        "        raw = self.ds.take([idx]).to_pylist()[0]\n",
        "        ins, inp, out = raw['instructions'], raw['inputs'], raw['outputs']\n",
        "\n",
        "        # Trim them so they are all 'cutoff' length long\n",
        "        ins, inp, out = self.trim(self.cutoff, self.pad_tok_id, ins, inp, out)\n",
        "\n",
        "        # Add the prompt's prefix, middle and suffix tokens to be in place\n",
        "        final_output = pre_tok + ins + mid_tok + inp + suf_tok + out\n",
        "        return final_output\n",
        "\n",
        "    def __len__(self):\n",
        "        # Since each row is a sample in our dataset, number of rows is number of samples\n",
        "        return self.ds.count_rows()\n",
        "\n",
        "    def trim(self, cutoff: int, pad_token: int, *args) -> list:\n",
        "        # Truncate (or pad) each passed-in list of tokens to the cutoff length\n",
        "        return [el[:cutoff] if len(el) >= cutoff else el+[pad_token]*(cutoff-len(el)) for el in args]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba6Nc_QWKLqh"
      },
      "outputs": [],
      "source": [
        "# Define the LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.2,\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXL9AF9gKM0f"
      },
      "outputs": [],
      "source": [
        "# Define the train and validation datasets\n",
        "# The datasets should be in the current directory in the current folder\n",
        "train_dataset = LanceDataset(\n",
        "    \"hindi_alpaca_dolly_train.lance/\",\n",
        ")\n",
        "valid_dataset = LanceDataset(\n",
        "    \"hindi_alpaca_dolly_val.lance/\",\n",
        ")\n",
        "\n",
        "# Define the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3rX_gpU8N2v"
      },
      "outputs": [],
      "source": [
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4, # change if your individual GPUs have more memory\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None, # remove this if you want to log to wandb\n",
        ")\n",
        "\n",
        "# Define the trainer and train and save the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(f\"{model_id.split('/')[-1]}-hindi\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
